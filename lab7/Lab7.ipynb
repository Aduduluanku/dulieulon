{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8HiMwYwfvXRJ"
      },
      "outputs": [],
      "source": [
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20cOcunQvXRL",
        "outputId": "288c85e1-1c4f-4e6c-d91b-b2a3b7c9762b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "\n",
        "nltk.download('gutenberg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whMTNCVRvXRN",
        "outputId": "ce58244d-edf9-443a-e94f-7645d77ada29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gutenberg files :  ['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
          ]
        }
      ],
      "source": [
        "\n",
        "gb = nltk.corpus.gutenberg\n",
        "\n",
        "print(\"Gutenberg files : \", gb.fileids())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "FNjGU7NRvXRN"
      },
      "outputs": [],
      "source": [
        "\n",
        "macbeth = nltk.corpus.gutenberg.words('shakespeare-macbeth.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pDjOxVYuvXRO",
        "outputId": "fc4ce117-d02f-4ea5-fe16-1ce45b8781fc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "23140"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "\n",
        "len(macbeth)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ld5n1MEvXRP",
        "outputId": "81bd8bce-58df-4223-aa13-52bd242d24d3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[',\n",
              " 'The',\n",
              " 'Tragedie',\n",
              " 'of',\n",
              " 'Macbeth',\n",
              " 'by',\n",
              " 'William',\n",
              " 'Shakespeare',\n",
              " '1603',\n",
              " ']']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "\n",
        "macbeth [:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jNqpyteevXRR",
        "outputId": "cc61bbb2-b8fd-4907-fce5-fae4e06f30d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Displaying 3 of 3 matches:\n",
            "nts with Dishes and Seruice ouer the Stage . Then enter Macbeth Macb . If it we\n",
            "with mans Act , Threatens his bloody Stage : byth ' Clock ' tis Day , And yet d\n",
            " struts and frets his houre vpon the Stage , And then is heard no more . It is \n"
          ]
        }
      ],
      "source": [
        "\n",
        "text = nltk.Text(macbeth)\n",
        "text.concordance('Stage')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_Tn8vCkvXRS",
        "outputId": "acbb64f3-df20-419c-b84a-04a41225d229"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the_. bloody_: the_,\n"
          ]
        }
      ],
      "source": [
        "\n",
        "text.common_contexts(['Stage'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "depIsi2VvXRT",
        "outputId": "27e910e0-7a2a-40a4-995d-946c98bdbb39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "day time face warre ayre king bleeding man reuolt serieant like\n",
            "knowledge broyle shew head spring heeles hare thane skie\n"
          ]
        }
      ],
      "source": [
        "\n",
        "text.similar('Stage')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4yKPyEmyvXRU",
        "outputId": "c22f634b-3307-4696-e2c5-2e1672b885b8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(',', 1962),\n",
              " ('.', 1235),\n",
              " (\"'\", 637),\n",
              " ('the', 531),\n",
              " (':', 477),\n",
              " ('and', 376),\n",
              " ('I', 333),\n",
              " ('of', 315),\n",
              " ('to', 311),\n",
              " ('?', 241)]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "\n",
        "fd = nltk.FreqDist(macbeth)\n",
        "fd.most_common(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xlhI4OSyvXRU",
        "outputId": "0d9b25b7-e3a2-400b-fae0-d27fe0c90811"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVGH2WMIvXRV",
        "outputId": "81f58e3c-a7da-4547-88b8-a52ffe1fb9d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "179\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['are',\n",
              " 'was',\n",
              " 'can',\n",
              " \"you'd\",\n",
              " 'here',\n",
              " 'shouldn',\n",
              " 'below',\n",
              " \"shouldn't\",\n",
              " 'y',\n",
              " 'once']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "\n",
        "sw = set(nltk.corpus.stopwords.words('english'))\n",
        "print(len(sw))\n",
        "list(sw)[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xDXD9ZiOvXRV",
        "outputId": "13345bb4-94cc-441e-a802-c4e746d553df"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14946"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "\n",
        "macbeth_filtered = [w for w in macbeth if w.lower() not in sw]\n",
        "len(macbeth_filtered)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fOlc76AvXRW",
        "outputId": "67479fb8-d79f-4d1b-9211-be1ef8385a1b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(',', 1962),\n",
              " ('.', 1235),\n",
              " (\"'\", 637),\n",
              " (':', 477),\n",
              " ('?', 241),\n",
              " ('Macb', 137),\n",
              " ('haue', 117),\n",
              " ('-', 100),\n",
              " ('Enter', 80),\n",
              " ('thou', 63)]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "\n",
        "fd = nltk.FreqDist(macbeth_filtered)\n",
        "fd.most_common(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VpFc2NMqvXRW",
        "outputId": "a9d662ff-763d-456a-ab7c-f429948a72f4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('macb', 137),\n",
              " ('haue', 122),\n",
              " ('thou', 90),\n",
              " ('enter', 81),\n",
              " ('shall', 68),\n",
              " ('macbeth', 62),\n",
              " ('vpon', 62),\n",
              " ('thee', 61),\n",
              " ('macd', 58),\n",
              " ('vs', 57)]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "\n",
        "import string\n",
        "punctuation = set(string.punctuation)\n",
        "macbeth_filtered2 = [w.lower() for w in macbeth if w.lower() not in sw and w.lower() not in punctuation]\n",
        "fd = nltk.FreqDist(macbeth_filtered2)\n",
        "fd.most_common(10)\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "213qUWKPvXRX",
        "outputId": "3c1c1431-0039-4329-ea4d-40923783b441"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Assassination',\n",
              " 'Chamberlaines',\n",
              " 'Distinguishes',\n",
              " 'Gallowgrosses',\n",
              " 'Metaphysicall',\n",
              " 'Northumberland',\n",
              " 'Voluptuousnesse',\n",
              " 'commendations',\n",
              " 'multitudinous',\n",
              " 'supernaturall',\n",
              " 'vnaccompanied']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "\n",
        "long_words = [w for w in macbeth if len(w)> 12]\n",
        "sorted(long_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B09I_bdavXRY",
        "outputId": "0a17f9a6-b016-4464-9f86-440cbd17a0b9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Auaricious',\n",
              " 'Gracious',\n",
              " 'Industrious',\n",
              " 'Iudicious',\n",
              " 'Luxurious',\n",
              " 'Malicious',\n",
              " 'Obliuious',\n",
              " 'Pious',\n",
              " 'Rebellious',\n",
              " 'compunctious',\n",
              " 'furious',\n",
              " 'gracious',\n",
              " 'pernicious',\n",
              " 'pernitious',\n",
              " 'pious',\n",
              " 'precious',\n",
              " 'rebellious',\n",
              " 'sacrilegious',\n",
              " 'serious',\n",
              " 'spacious',\n",
              " 'tedious']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "\n",
        "ious_words = [w for w in macbeth if 'ious' in w]\n",
        "ious_words = set(ious_words)\n",
        "sorted(ious_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51jZxB-EvXRY",
        "outputId": "42b310d3-54d6-4023-ee17-243bf43ff087"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(('enter', 'macbeth'), 16),\n",
              " (('exeunt', 'scena'), 15),\n",
              " (('thane', 'cawdor'), 13),\n",
              " (('knock', 'knock'), 10),\n",
              " (('st', 'thou'), 9),\n",
              " (('thou', 'art'), 9),\n",
              " (('lord', 'macb'), 9),\n",
              " (('haue', 'done'), 8),\n",
              " (('macb', 'haue'), 8),\n",
              " (('good', 'lord'), 8),\n",
              " (('let', 'vs'), 7),\n",
              " (('enter', 'lady'), 7),\n",
              " (('wee', 'l'), 7),\n",
              " (('would', 'st'), 6),\n",
              " (('macbeth', 'macb'), 6)]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "\n",
        "bgrms = nltk.FreqDist(nltk.bigrams(macbeth_filtered2))\n",
        "bgrms.most_common(15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJZ9RH2PvXRZ",
        "outputId": "47e44707-30e3-4b89-a689-251d4627a1d4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(('knock', 'knock', 'knock'), 6),\n",
              " (('enter', 'macbeth', 'macb'), 5),\n",
              " (('enter', 'three', 'witches'), 4),\n",
              " (('exeunt', 'scena', 'secunda'), 4),\n",
              " (('good', 'lord', 'macb'), 4),\n",
              " (('three', 'witches', '1'), 3),\n",
              " (('exeunt', 'scena', 'tertia'), 3),\n",
              " (('thunder', 'enter', 'three'), 3),\n",
              " (('exeunt', 'scena', 'quarta'), 3),\n",
              " (('scena', 'prima', 'enter'), 3)]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "tgrms = nltk.FreqDist(nltk.trigrams (macbeth_filtered2)) # nltk.trigrams: tạo ra các cặp ba từ liên tiếp trong văn bản\n",
        "tgrms.most_common(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "MZFVk4oAvXRZ",
        "outputId": "e51797bf-5c64-4b95-f2fa-84df60dd3cde"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ufeffThe Project Gutenberg eBook of Crime and Punishment, by Fyodor Dostoevsky\\r'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "\n",
        "from urllib import request\n",
        "url = \"http://www.gutenberg.org/files/2554/2554-0.txt\"\n",
        "response = request.urlopen(url)\n",
        "raw = response.read().decode('utf8')\n",
        "raw[:75]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "lrU-dAf6vXRZ",
        "outputId": "f258aae3-c808-45eb-bac3-69269a1bcd6f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The Project Gutenberg eBook of Crime and Punishment, by Fyodor Dostoevsky\\r\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "\n",
        "from urllib import request\n",
        "url = \"http://www.gutenberg.org/files/2554/2554-0.txt\"\n",
        "response = request.urlopen(url)\n",
        "raw = response.read().decode('utf-8-sig')\n",
        "raw[:75]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tBYnfNVgvXRa",
        "outputId": "0ffe79a1-1657-4e20-9e80-1a9a7362dc6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6ZHykHVvXRa",
        "outputId": "30eb631d-03ef-4d1a-aa86-0f3499b1a455"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The',\n",
              " 'Project',\n",
              " 'Gutenberg',\n",
              " 'eBook',\n",
              " 'of',\n",
              " 'Crime',\n",
              " 'and',\n",
              " 'Punishment',\n",
              " ',',\n",
              " 'by',\n",
              " 'Fyodor',\n",
              " 'Dostoevsky']"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "\n",
        "tokens = nltk.word_tokenize (raw)\n",
        "webtext = nltk.Text (tokens)\n",
        "webtext[:12]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "63Mb_vHhvXRa",
        "outputId": "40635e02-39be-4d15-b1af-d840e704c87d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<!DOCTYPE html>\\n<html class=\"client-nojs vector-feature-language-in-header-enabled vector-feature-language-in-main-page-'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "\n",
        "url = \"https://vi.wikipedia.org/wiki/Vi%E1%BB%87t_Nam\"\n",
        "html = request.urlopen(url).read().decode('utf8')\n",
        "html[:120]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "doEKh9dHvXRb"
      },
      "outputs": [],
      "source": [
        "\n",
        "from bs4 import BeautifulSoup\n",
        "raw = BeautifulSoup(html, \"html.parser\").get_text()\n",
        "tokens = nltk.word_tokenize(raw)\n",
        "text = nltk.Text(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eYeVwpqEvXRb",
        "outputId": "36100dc0-7857-46a7-d8d8-82b9b092b654"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "\n",
        "nltk.download('movie_reviews')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "lNr5fXEQvXRb"
      },
      "outputs": [],
      "source": [
        "\n",
        "import random\n",
        "reviews = nltk.corpus.movie_reviews\n",
        "documents = [(list(reviews.words(fileid)), category)\n",
        "             for category in reviews.categories()\n",
        "            for fileid in reviews.fileids(category)]\n",
        "random.shuffle(documents)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4DkijwLvXRc",
        "outputId": "7124b54f-668c-47c1-adc0-7f6da0630ef8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "if this keeps up , jane austen ( sense and sensibility , pride and prejudice ) may have to apply for posthumous membership to the screen writers guild . yet another novel of hers has made the transition to the silver screen . mansfield park is a turn of the century ( 18th century going on 19th ) story about love among the classes as well as an examination into proper society and family ties . ten year old fanny price ( hannah taylor - gordon , jakob the liar ) , taken from her mother and father and the poverty in which they dwell , is sent to live with her aunt and the privileged class at mansfield park , under the stern patriarchal hand of her uncle , sir thomas bertram ( harold pinter , mojo ) spending her days reminded of her lower status , she also invents and writes fanciful stories during her private times . fanny eventually grows to become a beautiful , intelligent , and engaging heroine ( quite unlike the original character which ms . austen originally penned in her novel . ) writer / director patricia rozema ( when night is falling ) is responsible for the textual changes . from a purely dramatic perspective , the revision makes perfect sense and improves the film ' s audience appeal . what ms . rozema has done is to infuse the main character with much of ms . austen ' s own personality by including excerpts from the author ' s journals , giving that dialogue to fanny . the result is a central character that is immediately appealing . as the grown fanny , australian actress frances o ' connor ( all about adam ) does wonderfully textured work . at times , ms . rozema has fanny address the camera directly to communicate many of the novel ' s more introspective observations . this is a difficult device to work seamlessly into a period film and it is to ms . o ' connor ' s credit that it works as well as it does . the central theme which gives the story its legs in an old one . . . whether it is better to marry for love or for social standing ? fanny has fallen in love with her cousin edmund ( jonny lee miller , plunkett & macleane ) who appears fond of her as well . his attentions are soon divided as the stylish and socially acceptable mary crawford ( embeth davidtz , bicentennial man ) enters the picture along with her equally acceptable brother henry ( alessandro nivola , inventing the abbotts ) who eventually sets his romantic sights upon fanny . while mary and henry are evidently less than sincere in their affections , their presence does provide the movie and the main characters with the necessary conflict that keeps our interest until the film ' s appropriately austen - like ending . other thematic devices include a awkwardly inserted reference to the source of the wealth of mansfield park . . . the slave trade . there is also a hint of both lesbianism and incest but neither is carried very far and is soon forgotten . the motivation for marriage remains the primary thematic thrust . fanny ' s cousin , maria bertram ( victoria hamilton , persuasion ) is an example of one making a poor match , marrying a well - to - do fool who is able to make her comfortable , but never happy . fanny ' s own mother , trapped in her chosen life of squalor warns fanny by admitting that her situation is due to the fact that she \" married for love . \" fanny , given those two terrible examples , and faced with the same choice is understandably indecisive as to which way to lean . the spiritual answer , of course , lies in the middle of those two extremes . marriage is not a cold , calculating decision based upon self - preservation . neither is it a senseless decision made in the warm afterglow of a passionate embrace . in the purest sense , marriage forms an insoluble union whereby two people agree to function as one . \" and said , for this cause shall a man leave father and mother , and shall cleave to his wife : and they twain shall be one flesh ? \" matthew 19 : 5 [ kjv ] love and logic can be combined . god ' s word contains both . so does a marriage based upon his truth .\n"
          ]
        }
      ],
      "source": [
        "\n",
        "first_review = ' '.join(documents[0][0])\n",
        "print(first_review)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "P63T_SnuvXRc",
        "outputId": "8ceb8dd8-e2f1-4326-b246-fe6e6aa4f300"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'pos'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "\n",
        "documents[0][1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "YBemavfFvXRc"
      },
      "outputs": [],
      "source": [
        "\n",
        "all_words = nltk.FreqDist(w.lower() for w in reviews.words())\n",
        "word_features = list(all_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "dQMv8iiyvXRh"
      },
      "outputs": [],
      "source": [
        "\n",
        "def document_features(document, word_features):\n",
        "    document_words = set(document)\n",
        "    features = {}\n",
        "    for word in word_features:\n",
        "        features['{}'.format(word)] = (word in document_words)\n",
        "    return features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Luqh8yAvXRh",
        "outputId": "0118225e-61cc-4e34-abf4-dd844220534d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2000"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "\n",
        "featuresets = [(document_features(d,word_features), c) for (d,c) in documents]\n",
        "len(featuresets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "NAUxBhrhvXRi"
      },
      "outputs": [],
      "source": [
        "\n",
        "train_set, test_set = featuresets[1500:], featuresets[:500]\n",
        "classifier = nltk.NaiveBayesClassifier.train(train_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dpoLvgk5vXRi",
        "outputId": "b5930a27-446c-4cc5-9a94-8550b2cb34d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.768\n"
          ]
        }
      ],
      "source": [
        "\n",
        "train_set, est_set = featuresets[1500:], featuresets[:500]\n",
        "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
        "print(nltk.classify.accuracy(classifier, test_set))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aot2xcFMvXRi",
        "outputId": "bfdbb834-2181-4197-965d-6f301df77c97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most Informative Features\n",
            "                  unless = True              neg : pos    =     13.0 : 1.0\n",
            "                   awful = True              neg : pos    =      9.8 : 1.0\n",
            "                destined = True              pos : neg    =      8.3 : 1.0\n",
            "                 dressed = True              neg : pos    =      8.3 : 1.0\n",
            "                    sick = True              neg : pos    =      8.3 : 1.0\n",
            "                   trade = True              pos : neg    =      8.3 : 1.0\n",
            "                  jackie = True              pos : neg    =      7.8 : 1.0\n",
            "                designer = True              pos : neg    =      7.7 : 1.0\n",
            "                 freedom = True              pos : neg    =      7.7 : 1.0\n",
            "                    mess = True              neg : pos    =      7.7 : 1.0\n"
          ]
        }
      ],
      "source": [
        "classifier.show_most_informative_features(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o5kKXIbvvXRj",
        "outputId": "e85724c6-7d10-4e29-a601-7a432d1f3034"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'book'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "NLTK corpus readers.  The modules in this package provide functions\n",
            "that can be used to read corpus files in a variety of formats.  These\n",
            "functions can be used to read both the corpus files that are\n",
            "distributed in the NLTK corpus package, and corpus files that are part\n",
            "of external corpora.\n",
            "\n",
            "Available Corpora\n",
            "=================\n",
            "\n",
            "Please see https://www.nltk.org/nltk_data/ for a complete list.\n",
            "Install corpora using nltk.download().\n",
            "\n",
            "Corpus Reader Functions\n",
            "=======================\n",
            "Each corpus module defines one or more \"corpus reader functions\",\n",
            "which can be used to read documents from that corpus.  These functions\n",
            "take an argument, ``item``, which is used to indicate which document\n",
            "should be read from the corpus:\n",
            "\n",
            "- If ``item`` is one of the unique identifiers listed in the corpus\n",
            "  module's ``items`` variable, then the corresponding document will\n",
            "  be loaded from the NLTK corpus package.\n",
            "- If ``item`` is a filename, then that file will be read.\n",
            "\n",
            "Additionally, corpus reader functions can be given lists of item\n",
            "names; in which case, they will return a concatenation of the\n",
            "corresponding documents.\n",
            "\n",
            "Corpus reader functions are named based on the type of information\n",
            "they return.  Some common examples, and their return types, are:\n",
            "\n",
            "- words(): list of str\n",
            "- sents(): list of (list of str)\n",
            "- paras(): list of (list of (list of str))\n",
            "- tagged_words(): list of (str,str) tuple\n",
            "- tagged_sents(): list of (list of (str,str))\n",
            "- tagged_paras(): list of (list of (list of (str,str)))\n",
            "- chunked_sents(): list of (Tree w/ (str,str) leaves)\n",
            "- parsed_sents(): list of (Tree with str leaves)\n",
            "- parsed_paras(): list of (list of (Tree with str leaves))\n",
            "- xml(): A single xml ElementTree\n",
            "- raw(): unprocessed corpus contents\n",
            "\n",
            "For example, to read a list of the words in the Brown Corpus, use\n",
            "``nltk.corpus.brown.words()``:\n",
            "\n",
            "    >>> from nltk.corpus import brown\n",
            "    >>> print(\", \".join(brown.words())) # doctest: +ELLIPSIS\n",
            "    The, Fulton, County, Grand, Jury, said, ...\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection book\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "\n",
        "nltk.download('book')\n",
        "\n",
        "\n",
        "print(nltk.corpus.__doc__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2E2We8YPvXRj",
        "outputId": "6666d00e-d02a-49ae-ebd1-e33ab72ea2fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**Danh sách stopwords trong ngôn ngữ english:\n",
            "179\n",
            "**Danh sách stopwords trong ngôn ngữ french:\n",
            "157\n",
            "**Danh sách stopwords trong ngôn ngữ german:\n",
            "232\n",
            "**Danh sách stopwords trong ngôn ngữ spanish:\n",
            "313\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "\n",
        "languages = ['english', 'french', 'german', 'spanish']\n",
        "\n",
        "for lang in languages:\n",
        "    print(f\"**Danh sách stopwords trong ngôn ngữ {lang}:\")\n",
        "    sw = set(nltk.corpus.stopwords.words(lang))\n",
        "    print(len(sw))\n",
        "    list(sw)[:10]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJmF95nbvXRk",
        "outputId": "1b2be0f7-31e3-477b-eb29-2ae43e9d1cb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**Kiểm tra các từ trong danh sách stopword của ngôn ngữ english:\n",
            "- 'the' là stopword trong ngôn ngữ english\n",
            "- 'et' không phải là stopword trong ngôn ngữ english\n",
            "- 'und' không phải là stopword trong ngôn ngữ english\n",
            "- 'y' là stopword trong ngôn ngữ english\n",
            "\n",
            "**Kiểm tra các từ trong danh sách stopword của ngôn ngữ french:\n",
            "- 'the' không phải là stopword trong ngôn ngữ french\n",
            "- 'et' là stopword trong ngôn ngữ french\n",
            "- 'und' không phải là stopword trong ngôn ngữ french\n",
            "- 'y' là stopword trong ngôn ngữ french\n",
            "\n",
            "**Kiểm tra các từ trong danh sách stopword của ngôn ngữ german:\n",
            "- 'the' không phải là stopword trong ngôn ngữ german\n",
            "- 'et' không phải là stopword trong ngôn ngữ german\n",
            "- 'und' là stopword trong ngôn ngữ german\n",
            "- 'y' không phải là stopword trong ngôn ngữ german\n",
            "\n",
            "**Kiểm tra các từ trong danh sách stopword của ngôn ngữ spanish:\n",
            "- 'the' không phải là stopword trong ngôn ngữ spanish\n",
            "- 'et' không phải là stopword trong ngôn ngữ spanish\n",
            "- 'und' không phải là stopword trong ngôn ngữ spanish\n",
            "- 'y' là stopword trong ngôn ngữ spanish\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "\n",
        "languages = ['english', 'french', 'german', 'spanish']\n",
        "words_to_check = ['the', 'et', 'und', 'y']\n",
        "\n",
        "for lang in languages:\n",
        "    print(f\"**Kiểm tra các từ trong danh sách stopword của ngôn ngữ {lang}:\")\n",
        "    sw = set(nltk.corpus.stopwords.words(lang))\n",
        "    for word in words_to_check:\n",
        "        if word.lower() in sw:\n",
        "            print(f\"- '{word}' là stopword trong ngôn ngữ {lang}\")\n",
        "        else:\n",
        "            print(f\"- '{word}' không phải là stopword trong ngôn ngữ {lang}\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-vz7qjavXRk",
        "outputId": "4d343901-a836-4c06-de07-e72b046e3b31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Văn bản sau khi loại bỏ các stop words:\n",
            "sample sentence , showing stop words filtration .\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "sample_text = \"This is a sample sentence, showing off the stop words filtration.\"\n",
        "\n",
        "\n",
        "tokens = word_tokenize(sample_text)\n",
        "\n",
        "\n",
        "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "filtered_text = [word for word in tokens if word.lower() not in stop_words]\n",
        "\n",
        "\n",
        "filtered_text = ' '.join(filtered_text)\n",
        "print(\"Văn bản sau khi loại bỏ các stop words:\")\n",
        "print(filtered_text)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}